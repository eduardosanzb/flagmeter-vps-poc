# Coolify deployment configuration for FlagMeter
# This file enables auto-detection and configuration in Coolify
#
# Optimized for: Hetzner CAX11 (ARM64, 2 vCPU, 4GB RAM)
# All images are ARM64 compatible

version: '1.0'

services:
  dashboard:
    build:
      dockerfile: infra/docker/Dockerfile.dashboard
      context: .
      args:
        BUILDKIT_INLINE_CACHE: 0
    ports:
      - 3000
    healthcheck:
      test: ["CMD-SHELL", "node -e 'http.get(\"http://localhost:3000/api/health\",(r)=>{process.exit(r.statusCode===200?0:1)})'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://flagmeter:flagmeter123@postgres:5432/flagmeter}
      VALKEY_URL: ${VALKEY_URL:-redis://valkey:6379}
      NODE_ENV: production
      QUEUE_NAME: events
      LOG_LEVEL: info
    depends_on:
      - postgres
      - valkey

  worker:
    build:
      dockerfile: infra/docker/Dockerfile.worker
      context: .
    environment:
      DATABASE_URL: ${DATABASE_URL}
      VALKEY_URL: ${VALKEY_URL}
      NODE_ENV: production
      WORKER_CONCURRENCY: 2
      QUEUE_NAME: events
      LOG_LEVEL: info
    depends_on:
      - postgres
      - valkey

  landing:
    build:
      dockerfile: Dockerfile
      context: apps/landing
    ports:
      - 80
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s

  postgres:
    image: postgres:18-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-flagmeter}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-flagmeter123}
      POSTGRES_DB: ${POSTGRES_DB:-flagmeter}

  valkey:
    image: valkey/valkey:7-alpine

  # Observability stack - commented out for POC, enable when needed
  # prometheus:
  #   image: prom/prometheus:latest
  #   ports:
  #     - 9090
  #   volumes:
  #     - prometheus_data:/prometheus
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'

  # grafana:
  #   image: grafana/grafana:latest
  #   ports:
  #     - 3001
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   environment:
  #     GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER:-admin}
  #     GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD:-admin}

  # loki:
  #   image: grafana/loki:latest
  #   ports:
  #     - 3100
  #   volumes:
  #     - loki_data:/loki

volumes:
  postgres_data:
  # prometheus_data:
  # grafana_data:
  # loki_data:

# Preview deployments configuration
previews:
  enabled: true
  pattern: pr-{number}
  domain: meter.yourdomain.com
  services:
    - dashboard
    - worker

# Resource limits optimized for Hetzner CAX11 (ARM64, 2 vCPU, 4GB RAM)
resources:
  dashboard:
    memory: 1024M     # TanStack Start SSR + API routes
    cpu: 0.75
  worker:
    memory: 768M      # Event processing queue
    cpu: 0.5
  postgres:
    memory: 1024M     # Main data store
    cpu: 0.5
  valkey:
    memory: 384M      # Event queue + cache
    cpu: 0.25
  landing:
    memory: 128M      # Static nginx site (minimal resources)
    cpu: 0.1
  # Observability services commented out for POC
  # prometheus:
  #   memory: 384M
  #   cpu: 0.25
  # grafana:
  #   memory: 384M
  #   cpu: 0.25
  # loki:
  #   memory: 256M
  #   cpu: 0.25
# Total with core services only: ~3.3GB RAM allocated, ~2.1GB CPU shares
# Leaves ~700MB for OS and buffers
