# Coolify deployment configuration for FlagMeter
# This file enables auto-detection and configuration in Coolify
#
# Optimized for: Hetzner CAX11 (ARM64, 2 vCPU, 4GB RAM)
# All images are ARM64 compatible

version: '1.0'

services:
  dashboard:
    build:
      dockerfile: infra/docker/Dockerfile.dashboard
      context: .
      args:
        BUILDKIT_INLINE_CACHE: 0
    ports:
      - 3000
    healthcheck:
      test: ["CMD-SHELL", "node -e 'http.get(\"http://localhost:3000/api/health\",(r)=>{process.exit(r.statusCode===200?0:1)})'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://flagmeter:flagmeter123@postgres:5432/flagmeter}
      VALKEY_URL: ${VALKEY_URL:-redis://valkey:6379}
      LOKI_URL: ${LOKI_URL:-http://loki:3100}
      NODE_ENV: production
      QUEUE_NAME: events
      LOG_LEVEL: info
    depends_on:
      - postgres
      - valkey

  worker:
    build:
      dockerfile: infra/docker/Dockerfile.worker
      context: .
    environment:
      DATABASE_URL: ${DATABASE_URL}
      VALKEY_URL: ${VALKEY_URL}
      LOKI_URL: ${LOKI_URL:-http://loki:3100}
      NODE_ENV: production
      WORKER_CONCURRENCY: ${WORKER_CONCURRENCY:-4}  # Increased for load testing (was 2)
      QUEUE_NAME: events
      LOG_LEVEL: info
    depends_on:
      - postgres
      - valkey

  postgres:
    image: postgres:18-alpine
    command: >
      postgres
      -c shared_buffers=1GB
      -c effective_cache_size=4GB
      -c maintenance_work_mem=256MB
      -c work_mem=16MB
      -c wal_buffers=16MB
      -c max_wal_size=3GB
      -c min_wal_size=1GB
      -c checkpoint_timeout=900
      -c checkpoint_completion_target=0.9
      -c wal_compression=on
      -c synchronous_commit=off
      -c wal_writer_delay=200ms
      -c bgwriter_delay=200ms
      -c bgwriter_lru_maxpages=100
      -c bgwriter_lru_multiplier=2.0
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c max_connections=100
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c autovacuum=on
      -c autovacuum_naptime=30s
      -c autovacuum_vacuum_cost_limit=1000
      -c autovacuum_max_workers=3
      -c autovacuum_vacuum_scale_factor=0.1
      -c autovacuum_analyze_scale_factor=0.05
      -c log_checkpoints=on
      -c log_lock_waits=on
      -c log_temp_files=0
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c pg_stat_statements.max=10000
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-flagmeter}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-flagmeter123}
      POSTGRES_DB: ${POSTGRES_DB:-flagmeter}

  valkey:
    image: valkey/valkey:7-alpine

  # Observability stack
  prometheus:
    build:
      dockerfile: infra/docker/Dockerfile.prometheus
      context: .
    ports:
      - 9090
    volumes:
      - prometheus_data:/prometheus
    depends_on:
      - dashboard
      - worker

  grafana:
    build:
      dockerfile: infra/docker/Dockerfile.grafana
      context: .
    ports:
      - 3001
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SERVER_ROOT_URL: ${GF_SERVER_ROOT_URL:-https://grafana.raus.cloud}
      GF_FEATURE_TOGGLES_ENABLE: publicDashboards
      GF_ALLOW_EMBEDDING: true
    depends_on:
      - prometheus
      - loki

  loki:
    image: grafana/loki:latest
    ports:
      - 3100
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki_data:/loki

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      DATA_SOURCE_NAME: postgresql://${POSTGRES_USER:-flagmeter}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-flagmeter}?sslmode=disable
    depends_on:
      - postgres

  redis-exporter:
    image: oliver006/redis_exporter:latest
    environment:
      REDIS_ADDR: valkey:6379
    depends_on:
      - valkey

volumes:
  postgres_data:
  prometheus_data:
  grafana_data:
  loki_data:

# Preview deployments configuration
previews:
  enabled: true
  pattern: pr-{number}
  domain: meter.yourdomain.com
  services:
    - dashboard
    - worker

# Resource limits optimized for Hetzner CAX11 (ARM64, 2 vCPU, 4GB RAM)
# Based on actual load testing metrics
resources:
  dashboard:
    memory: 1024M     # TanStack Start SSR + API routes
    cpu: 0.7          # Primary ingestion endpoint
  worker:
    memory: 768M      # I/O bound async processing with 4 workers
    cpu: 0.3          # Not CPU intensive (BRPOP + async DB queries)
  postgres:
    memory: 1024M     # Main data store + 4 worker connections
    cpu: 0.5          # Query planning/execution
  valkey:
    memory: 128M      # Actual usage: ~6MB in prod, 128M = 20x headroom
    cpu: 0.05         # Minimal CPU for queue ops
  prometheus:
    memory: 256M      # Metrics storage and queries
    cpu: 0.2
  grafana:
    memory: 256M      # Dashboard visualization
    cpu: 0.2
  loki:
    memory: 128M      # Log aggregation
    cpu: 0.1
  postgres-exporter:
    memory: 64M       # PostgreSQL metrics exporter
    cpu: 0.05
  redis-exporter:
    memory: 64M       # Valkey/Redis metrics exporter
    cpu: 0.05
# Total with observability: ~3.7GB RAM allocated, ~2.25 CPU shares
# Valkey optimized based on prod metrics (6MB actual usage)
